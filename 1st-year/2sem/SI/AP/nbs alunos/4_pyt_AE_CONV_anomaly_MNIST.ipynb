{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Autoencoder simples CONV para deteção de anomalias\n",
    "Neste caso, para deteção de imagens que não pertencem o MNIST dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Normal Data*\n",
    "\n",
    "\n",
    "![image_info](https://miro.medium.com/max/1400/1*P7aFcjaMGLwzTvjW3sD-5Q.jpeg)\n",
    "\n",
    "\n",
    "*Anomaly*\n",
    "\n",
    "\n",
    "![image_info](https://miro.medium.com/max/1400/1*-bHmKt4JPKIAmmyO47OYnQ.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pytorch conv for binary classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "    \n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "#path para guardar o dataset\n",
    "PATH = './'\n",
    "PATH_TRAIN = './mnist_train.csv'\n",
    "PATH_TEST = './mnist_test.csv'\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device management \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Preparar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#buscar o dataset utilizando os CSVs e uma classe para o dataset\n",
    "\n",
    "# definição classe para o dataset\n",
    "class CSVDataset(Dataset):\n",
    "    # ler o dataset\n",
    "    def __init__(self, path_train, path_test):\n",
    "        # ler o ficheiro csv para um dataframe\n",
    "        df_train = pd.read_csv(path_train, header=0)\n",
    "        df_test = pd.read_csv(path_test, header=0)\n",
    "        # separar os inputs e os outputs\n",
    "        self.x_train = df_train.values[:, 1:]\n",
    "        self.x_train = self.x_train.reshape(len(self.x_train), 1, 28, 28)\n",
    "        xmax, xmin = self.x_train.max(), self.x_train.min()\n",
    "        self.x_train  = (self.x_train - xmin)/(xmax - xmin)\n",
    "        self.y_train = df_train.values[:, 0]\n",
    "        self.x_test = df_test.values[:, 1:]\n",
    "        self.x_test = self.x_test.reshape(len(self.x_test), 1, 28, 28)\n",
    "        xmax, xmin = self.x_test.max(), self.x_test.min()\n",
    "        self.x_test  = (self.x_test - xmin)/(xmax - xmin)\n",
    "        self.y_test = df_test.values[:, 0]\n",
    "        # garantir que os inputs e labels sejam floats\n",
    "        self.x_train = self.x_train.astype('float32')\n",
    "        self.x_test = self.x_test.astype('float32')\n",
    "        self.y_train = self.y_train.astype('long')\n",
    "        self.y_test = self.y_test.astype('long')\n",
    "        \n",
    "    # numero de casos de treino no dataset\n",
    "    def __len_train__(self):\n",
    "        return len(self.x_train)\n",
    "     # numero de casos de teste no dataset\n",
    "    def __len_test__(self):\n",
    "        return len(self.x_test)\n",
    "    \n",
    "    # retornar um caso\n",
    "    def __getitem_train__(self, idx):\n",
    "        return [self.x_train[idx], self.y_train[idx]]\n",
    "     # retornar um caso\n",
    "    def __getitem_test__(self, idx):\n",
    "        return [self.x_test[idx], self.y_test[idx]]\n",
    "    \n",
    "    # retornar indeces para casos de treino de de teste em formato flat (vetor)\n",
    "    def get_splits(self):\n",
    "        x_train  = torch.from_numpy(np.array(self.x_train))\n",
    "        y_train  = torch.from_numpy(np.array(self.y_train))\n",
    "        x_test  = torch.from_numpy(np.array(self.x_test))\n",
    "        y_test  = torch.from_numpy(np.array(self.y_test))\n",
    "        train = torch.utils.data.TensorDataset(x_train,y_train)\n",
    "        test = torch.utils.data.TensorDataset(x_test,y_test)\n",
    "        return train, test \n",
    "    \n",
    "# preparar o dataset\n",
    "def prepare_data_flat(path_train, path_test):\n",
    "    # criar uma instancia do dataset\n",
    "    dataset = CSVDataset(path_train, path_test)\n",
    "    # calcular split\n",
    "    train, test = dataset.get_splits()\n",
    "    # preparar data loaders\n",
    "    train_dl = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    train_dl_all = DataLoader(train, batch_size=len(train), shuffle=False)\n",
    "    test_dl_all = DataLoader(test, batch_size=len(test), shuffle=False)\n",
    "    return train_dl, test_dl, train_dl_all, test_dl_all\n",
    "\n",
    "# preparar os dados\n",
    "train_dl, test_dl,  train_dl_all, test_dl_all = prepare_data_flat(PATH_TRAIN, PATH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Visualizar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualização das imagens\n",
    "def visualize_mnist_images_flat(dl):\n",
    "    # get one batch of images\n",
    "    i, (inputs, targets) = next(enumerate(dl))\n",
    "    print(inputs.shape)\n",
    "    print(inputs.shape)\n",
    "    print(inputs.shape)\n",
    "    # plot some images\n",
    "    plt.figure(figsize=(8,8))\n",
    "    for i in range(25):\n",
    "        # define subplot\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.grid(b=None)\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(inputs[i][0], cmap='gray')\n",
    "    # show the figure\n",
    "    plt.show()\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Ler o modelo previamente treinado em \"2_pytorch_AE_CONV_treino_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import models_mnist #modulo python com os modelos       \n",
    "    \n",
    "# definir a rede neuronal\n",
    "model = models_mnist.AE_CONV()\n",
    "\n",
    "# ler o modelo\n",
    "SAVED_MODEL = ...\n",
    "#model= torch.load(SAVED_MODEL)\n",
    "model= torch.load(SAVED_MODEL, map_location ='cpu')\n",
    "model.eval()\n",
    "#visualizar a rede\n",
    "print(summary(model, input_size=(BATCH_SIZE,  1,28,28), verbose=0))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Usar o Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podemos utilizar este modelo para deteção de anomalias (imagens que não são digitos)\n",
    "\n",
    "# Processar a imagem\n",
    "def process_image(image_path,w,h):\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    # Resize para alteração da dimensão mas a manter o aspect ratio\n",
    "    img = img.resize((w, int(h*(height/width))) if width < height else (int(w*(width/height)), h))\n",
    "    # nbter as dimensões novas\n",
    "    width, height = img.size\n",
    "    # Definir as coordenadas para o centro de w x h\n",
    "    left = (width - w)/2\n",
    "    top = (height - h)/2\n",
    "    right = (width + w)/2\n",
    "    bottom = (height + h)/2\n",
    "    img = img.crop((left, top, right, bottom))\n",
    "    img = ImageOps.grayscale(img)\n",
    "    # Converter para array numpy\n",
    "    img = np.array(img)\n",
    "    print(f'shape:{img.shape}')\n",
    "    # Normalizar\n",
    "    xmax, xmin = img.max(), img.min()\n",
    "    img  = (img - xmin)/(xmax - xmin)\n",
    "    # Adicionar uma quarta dimensão ao início para indicar o batch size\n",
    "    img = img[np.newaxis,:]\n",
    "    # Converter num tensor torch\n",
    "    image = torch.from_numpy(img)\n",
    "    image = image.float()\n",
    "    #image=image.view(1,w*h) #fazer o flat do 28x28 para ficar como o mnist\n",
    "    return image\n",
    "\n",
    "def anomaly_detection(model, img_anomaly, img_list, idx): #img shape (784,1)\n",
    "    print(img_list.shape)\n",
    "    print(img_list.dtype) \n",
    "    img_list = img_list.to(device)\n",
    "    img_anomaly= img_anomaly.to(device)   \n",
    "    pred_img_anomaly,_ = model(img_anomaly)\n",
    "    print(f'img_anomaly.shape: {img_anomaly.shape}')\n",
    "    print(f'pred_img_anomaly.shape: {pred_img_anomaly.shape}')\n",
    "    dist_pred_img = np.linalg.norm(img_anomaly[0].cpu().detach().numpy() - pred_img_anomaly[0].cpu().detach().numpy())  #Distancia de não digito: 22.185663\n",
    "    print(\"Distancia de não digito:\",dist_pred_img)    \n",
    "    pred_img_list,_ = model(img_list)\n",
    "    print(f'pred_img_list.shape: {pred_img_list[idx].shape}')\n",
    "    dist_img1 = np.linalg.norm(img_list[idx].cpu().detach().numpy() - pred_img_list[idx].cpu().detach().numpy())  #Distancia de não digito: 22.185663\n",
    "    print(\"Distancia de  digito1:\",dist_img1)  \n",
    "    \n",
    "    \n",
    "ANOMALIA = ...\n",
    "#ANOMALIA = 'mnist_reconstruction_in.png'\n",
    "\n",
    "img = process_image(ANOMALIA,28,28)\n",
    "print(f'img.shape: {img.shape}')\n",
    "_, (inputs, targets) = next(enumerate(test_dl))\n",
    "# se a imagem imagem_nao_digito.png não for um digito do genero em que foi treinado então a distancia entre os dois vetores será muito grande.\n",
    "anomaly_detection(model, img, inputs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
